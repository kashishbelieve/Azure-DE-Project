# Azure-DE-Project-Resources
# Project Overview
This project demonstrates how to design and implement a data pipeline using Azure Data Factory and Databricks, with a focus on:
Data governance using Unity Catalog and Delta Lake.
Implementation of the Medallion architecture for optimal data organization.
Data modeling techniques such as Dimensional Data Modeling and Slowly Changing Dimensions (SCD).
Optimizing data workflows and performance with PySpark and Delta Tables.
# Technologies Used
Azure Data Factory: For orchestrating data workflows and pipelines.
Azure Databricks: For data processing using PySpark and leveraging Delta Lake for storage.
PySpark: For distributed data processing and transformation.
Unity Catalog: For data governance and metadata management.
Delta Lake: For providing reliable data storage and ACID transactions.
Medallion Architecture: For organizing the data pipeline into multiple stages for better scalability and maintainability.
# Features
Data Governance: Implemented using Unity Catalog for metadata management and enforcing access control.
Data Modeling: Applied Dimensional Data Modeling and Slowly Changing Dimensions to design a scalable data warehouse.
Real-Time Data Processing: Leveraged PySpark to process large datasets and perform data transformations efficiently.
Optimized Data Storage: Utilized Delta Lake to store data in a fault-tolerant manner while ensuring high-performance querying.
ETL Workflow: Designed a fully automated ETL pipeline using Azure Data Factory
